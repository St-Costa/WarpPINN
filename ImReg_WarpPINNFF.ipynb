{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19840,"status":"ok","timestamp":1679324132056,"user":{"displayName":"Pablo Arratia Lopez","userId":"17856875456012334674"},"user_tz":0},"id":"fAMRiM5Oatxs","outputId":"79b8aec9-0a63-4a73-b85f-2465d480ae07"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":746,"status":"ok","timestamp":1679324132791,"user":{"displayName":"Pablo Arratia Lopez","userId":"17856875456012334674"},"user_tz":0},"id":"F4QCBq5Ibq0t","outputId":"588d97f4-1b0c-48a0-cedb-572872e66940"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab_Notebooks/MRI_data/Code\n"]}],"source":["%cd /content/drive/MyDrive/Colab_Notebooks/MRI_data/Code"]},{"cell_type":"code","source":["!pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KLCK6hW9_BwS","executionInfo":{"status":"ok","timestamp":1679324177969,"user_tz":0,"elapsed":45181,"user":{"displayName":"Pablo Arratia Lopez","userId":"17856875456012334674"}},"outputId":"720b461b-9e7d-4d53-9af8-6585775e3263"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyDOE\n","  Downloading pyDOE-0.3.8.zip (22 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting numpy==1.20.1\n","  Downloading numpy-1.20.1-cp39-cp39-manylinux2010_x86_64.whl (15.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting meshio==4.4.6\n","  Downloading meshio-4.4.6-py3-none-any.whl (158 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.1/158.1 KB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nilearn==0.8.1\n","  Downloading nilearn-0.8.1-py3-none-any.whl (10.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nibabel==3.2.1\n","  Downloading nibabel-3.2.1-py3-none-any.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting vtk==9.0.3\n","  Downloading vtk-9.0.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (59.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pyvista==0.32.1\n","  Downloading pyvista-0.32.1-py3-none-any.whl (1.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydicom==2.1.2\n","  Downloading pydicom-2.1.2-py3-none-any.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2 in /usr/local/lib/python3.9/dist-packages (from nilearn==0.8.1->-r requirements.txt (line 4)) (2.27.1)\n","Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.9/dist-packages (from nilearn==0.8.1->-r requirements.txt (line 4)) (1.10.1)\n","Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.9/dist-packages (from nilearn==0.8.1->-r requirements.txt (line 4)) (1.2.2)\n","Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.9/dist-packages (from nilearn==0.8.1->-r requirements.txt (line 4)) (1.1.1)\n","Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.9/dist-packages (from nilearn==0.8.1->-r requirements.txt (line 4)) (1.4.4)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.9/dist-packages (from nibabel==3.2.1->-r requirements.txt (line 5)) (23.0)\n","Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from vtk==9.0.3->-r requirements.txt (line 6)) (3.7.1)\n","Collecting wslink>=0.1.3\n","  Downloading wslink-1.10.1-py3-none-any.whl (28 kB)\n","Collecting autobahn>=17.7.1\n","  Downloading autobahn-23.1.2.tar.gz (480 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.7/480.7 KB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting Twisted>=17.5.0\n","  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: imageio in /usr/local/lib/python3.9/dist-packages (from pyvista==0.32.1->-r requirements.txt (line 7)) (2.9.0)\n","Collecting appdirs\n","  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from pyvista==0.32.1->-r requirements.txt (line 7)) (8.4.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from pyvista==0.32.1->-r requirements.txt (line 7)) (4.5.0)\n","Collecting scooby>=0.5.1\n","  Downloading scooby-0.7.1-py3-none-any.whl (16 kB)\n","Collecting txaio>=21.2.1\n","  Downloading txaio-23.1.1-py2.py3-none-any.whl (30 kB)\n","Collecting cryptography>=3.4.6\n","  Downloading cryptography-39.0.2-cp36-abi3-manylinux_2_28_x86_64.whl (4.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting hyperlink>=21.0.0\n","  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from autobahn>=17.7.1->vtk==9.0.3->-r requirements.txt (line 6)) (63.4.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->vtk==9.0.3->-r requirements.txt (line 6)) (1.0.7)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->vtk==9.0.3->-r requirements.txt (line 6)) (2.8.2)\n","Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->vtk==9.0.3->-r requirements.txt (line 6)) (5.12.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->vtk==9.0.3->-r requirements.txt (line 6)) (3.0.9)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->vtk==9.0.3->-r requirements.txt (line 6)) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->vtk==9.0.3->-r requirements.txt (line 6)) (4.39.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.0.0->vtk==9.0.3->-r requirements.txt (line 6)) (1.4.4)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->nilearn==0.8.1->-r requirements.txt (line 4)) (2022.7.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2->nilearn==0.8.1->-r requirements.txt (line 4)) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2->nilearn==0.8.1->-r requirements.txt (line 4)) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2->nilearn==0.8.1->-r requirements.txt (line 4)) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2->nilearn==0.8.1->-r requirements.txt (line 4)) (2.0.12)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21->nilearn==0.8.1->-r requirements.txt (line 4)) (3.1.0)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.9/dist-packages (from Twisted>=17.5.0->vtk==9.0.3->-r requirements.txt (line 6)) (22.2.0)\n","Collecting incremental>=21.3.0\n","  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n","Collecting constantly>=15.1\n","  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n","Collecting Automat>=0.8.0\n","  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n","Collecting zope.interface>=4.4.2\n","  Downloading zope.interface-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (246 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.1/246.1 KB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiohttp<4\n","  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from Automat>=0.8.0->Twisted>=17.5.0->vtk==9.0.3->-r requirements.txt (line 6)) (1.15.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.9/dist-packages (from cryptography>=3.4.6->autobahn>=17.7.1->vtk==9.0.3->-r requirements.txt (line 6)) (1.15.1)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib>=2.0.0->vtk==9.0.3->-r requirements.txt (line 6)) (3.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.12->cryptography>=3.4.6->autobahn>=17.7.1->vtk==9.0.3->-r requirements.txt (line 6)) (2.21)\n","Building wheels for collected packages: pyDOE, autobahn\n","  Building wheel for pyDOE (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyDOE: filename=pyDOE-0.3.8-py3-none-any.whl size=18184 sha256=913d363dda4a69687396f68ed61b8789fe459810644d786cc23eabd1eddc55d8\n","  Stored in directory: /root/.cache/pip/wheels/3c/ca/0d/63b767ad585fbcb3ea222541572dcb78859d0383510672b105\n","  Building wheel for autobahn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for autobahn: filename=autobahn-23.1.2-cp39-cp39-linux_x86_64.whl size=707053 sha256=78fa16220725fe4686cc2f01caa0bc25f85be54a2f92cfa27eb878bcd56831c5\n","  Stored in directory: /root/.cache/pip/wheels/40/b2/da/d39c6c5ff78abfd09d37663025e174f8920ca01e3d6eccecd9\n","Successfully built pyDOE autobahn\n","Installing collected packages: incremental, constantly, appdirs, zope.interface, txaio, scooby, pydicom, numpy, multidict, hyperlink, frozenlist, Automat, async-timeout, yarl, Twisted, nibabel, meshio, cryptography, aiosignal, pyDOE, autobahn, aiohttp, wslink, nilearn, vtk, pyvista\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.22.4\n","    Uninstalling numpy-1.22.4:\n","      Successfully uninstalled numpy-1.22.4\n","  Attempting uninstall: nibabel\n","    Found existing installation: nibabel 3.0.2\n","    Uninstalling nibabel-3.0.2:\n","      Successfully uninstalled nibabel-3.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","librosa 0.10.0 requires numpy>=1.20.3, but you have numpy 1.20.1 which is incompatible.\n","cmdstanpy 1.1.0 requires numpy>=1.21, but you have numpy 1.20.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed Automat-22.10.0 Twisted-22.10.0 aiohttp-3.8.4 aiosignal-1.3.1 appdirs-1.4.4 async-timeout-4.0.2 autobahn-23.1.2 constantly-15.1.0 cryptography-39.0.2 frozenlist-1.3.3 hyperlink-21.0.0 incremental-22.10.0 meshio-4.4.6 multidict-6.0.4 nibabel-3.2.1 nilearn-0.8.1 numpy-1.20.1 pyDOE-0.3.8 pydicom-2.1.2 pyvista-0.32.1 scooby-0.7.1 txaio-23.1.1 vtk-9.0.3 wslink-1.10.1 yarl-1.8.2 zope.interface-6.0\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":8337,"status":"ok","timestamp":1679324186300,"user":{"displayName":"Pablo Arratia Lopez","userId":"17856875456012334674"},"user_tz":0},"id":"KFaFBUusZKwW"},"outputs":[],"source":["import numpy as np\n","import nibabel as nib\n","from aux_functions_time import *\n","from WarpPINN import WarpPINN\n","import time\n","import os"]},{"cell_type":"markdown","metadata":{"id":"fpRTz16QZODf"},"source":["# Global Variables"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1679324186301,"user":{"displayName":"Pablo Arratia Lopez","userId":"17856875456012334674"},"user_tz":0},"id":"aXvBTM7WZQAn"},"outputs":[],"source":["volunteers = [\"v1\",\"v2\",\"v4\",\"v5\",\"v6\",\"v7\",\"v8\",\"v9\",\"v10\",\"v11\",\"v12\",\"v13\",\"v14\",\"v15\",\"v16\"]\n","\n","# Images are cropped around the left ventricle since we are interested in this region.\n","crop_list = [[90, 190, 70, 170], [80, 170, 85, 175], [95, 185, 65, 155], [90, 180, 80, 170],\n","            [70, 160, 80, 170], [100, 200, 100, 200], [130, 220, 80, 170], [60, 160, 100, 190],\n","            [80, 160, 50, 140], [100, 200, 70, 170], [70, 160, 100, 190], [80, 170, 80, 170],\n","            [80, 170, 60, 150], [50, 140, 90, 180], [60, 150, 60, 150]] \n","\n","# Images need to be flipped\n","flip_list = [ [], [], [2], [2,3], [2,3], [2,3], [2], [], [2,3], [2,3], [3], [2,3], [2,3], [2,3], [2,3]]\n","\n","# Choose volunteer\n","idx_vol = 0\n","volunteer = volunteers[idx_vol]"]},{"cell_type":"markdown","metadata":{"id":"1rV9A72gZT4p"},"source":["# Loading a bunch of data"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16413,"status":"ok","timestamp":1679324202702,"user":{"displayName":"Pablo Arratia Lopez","userId":"17856875456012334674"},"user_tz":0},"id":"vquTAjysZKwZ","outputId":"6fba028b-4384-4f5b-e908-38b5bb7618a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device mapping: no known devices.\n"]}],"source":["data_path = os.path.join( 'data', volunteer) # For cluster\n","\n","# Image\n","imt_path = os.path.join(data_path,'imt.nii.gz')\n","\n","if os.path.exists(imt_path):\n","    imt_data = nib.load( imt_path )\n","else:\n","    imt_data = create_stack(data_path)\n","\n","header = imt_data.header\n","pix_dim = header['pixdim']\n","\n","# Pixel spacing of image\n","pixsp_x = pix_dim[1]\n","pixsp_y = pix_dim[2]\n","pixsp_z = pix_dim[3]\n","\n","# Crop\n","crop_x_in = crop_list[idx_vol][0]\n","crop_x_end = crop_list[idx_vol][1]\n","crop_y_in = crop_list[idx_vol][2]\n","crop_y_end = crop_list[idx_vol][3]\n","\n","crop_str = str(crop_x_in)+'_'+str(crop_x_end)+'_'+str(crop_y_in)+'_'+str(crop_y_end)\n","\n","# Getting all the images\n","# Dimension of the stack: [time x slice x X x Y]\n","imt = imt_data.get_fdata()\n","imt = np.transpose(imt, [3, 2, 1, 0])\n","\n","imt = np.flip(imt, flip_list[idx_vol]) \n","imt = imt[:, :, crop_y_in:crop_y_end, crop_x_in:crop_x_end].astype(np.float32)\n","\n","# Reference image is the first time in the stack\n","imr = imt[0, :, :, :]\n","\n","frames = imt.shape[0]\n","slices = imt.shape[1]\n","\n","# Mesh coords of image voxels and segmentation nodes\n","im_mesh, segm_mesh = im_and_segm_mesh_hernan(crop_x_in, crop_x_end, crop_y_in, crop_y_end, data_path)\n","\n","# Mesh coords for background nodes\n","bg_mesh_file = os.path.join(data_path, 'background_points_'+crop_str+'.npy')\n","bg_mesh = background_mesh(data_path, imt_data, crop_x_in, crop_x_end, crop_y_in, crop_y_end, bg_mesh_file, slices)\n","\n","# Boolean mask: 1 if the voxel is in the LV, 0 otherwise\n","bool_mask_file = os.path.join(data_path, 'boolean_mask_'+crop_str+'.npy')\n","bool_mask = boolean_mask(data_path, imt_data, crop_x_in, crop_x_end, crop_y_in, crop_y_end, bool_mask_file )\n","\n","### Neural Network ###\n","\n","# Fourier Features\n","m = 2**3\n","sigma = [1]\n","N_s = len(sigma)\n","\n","# Layers for u1, u2 and u3\n","neu = 2**6\n","layers_u = [2*m+1, neu, neu, neu, neu, N_s*neu, 3]\n","\n","reg_mask = 1\n","lmbmu = 10**5\n","mu_NeoHook = 1e-5\n","\n","# Number of iterations desired for the registration from t_1 to t_i\n","It = 300000\n","batch_size = 1000\n","# Number of points in LV used to calculate the Neo Hookean by minibatch\n","N_nodes = len(segm_mesh)\n","# Number of epochs \n","nEpoch = int( np.ceil(It * batch_size / N_nodes) )\n","\n","pix_crop = [pixsp_x, pixsp_y, pixsp_z, crop_x_in, crop_y_in]\n","\n","model = WarpPINN(imr, imt, layers_u, bool_mask, im_mesh, segm_mesh, bg_mesh, lmbmu, pix_crop, reg_mask = reg_mask, sigma = sigma)\n"]},{"cell_type":"markdown","metadata":{"id":"A-SmmWazZbYC"},"source":["# Pretrain model\n","\n","Once initialised, the network is first pre-trained to get $u_{\\theta}(x)\\sim 0$, then the deformation field approximates the identity  $\\varphi(X) = X + u_{\\theta}(X) \\sim X$"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":450},"executionInfo":{"elapsed":11116,"status":"error","timestamp":1679324213815,"user":{"displayName":"Pablo Arratia Lopez","userId":"17856875456012334674"},"user_tz":0},"id":"uFgRkRllZhH8","outputId":"0e2aa0e3-5850-49df-a9b2-1f80d7c0981b"},"outputs":[{"output_type":"stream","name":"stdout","text":["It: 0, Loss: 2.545e-01, Time: 2.21\n","It: 1, Loss: 9.034e-02, Time: 1.86\n","It: 2, Loss: 1.019e-01, Time: 1.88\n","It: 3, Loss: 1.501e-01, Time: 1.70\n","It: 4, Loss: 1.594e-01, Time: 1.69\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-7a518fec173b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtol_pretrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtol_pretrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/Colab_Notebooks/MRI_data/Code/WarpPINN.py\u001b[0m in \u001b[0;36mpretrain\u001b[0;34m(self, tol)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mtf_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_tf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op_Adam_pretrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m             \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprelossit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    969\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    970\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[1;32m   1192\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[1;32m   1372\u001b[0m                            run_metadata)\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1378\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1359\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[1;32m   1362\u001b[0m                                       target_list, run_metadata)\n\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1452\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1453\u001b[0m                           run_metadata):\n\u001b[0;32m-> 1454\u001b[0;31m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[1;32m   1455\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m                                             run_metadata)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["t0 = time.time()\n","tol_pretrain = 1e-6\n","model.pretrain(tol_pretrain)\n","t1 = time.time()"]},{"cell_type":"markdown","metadata":{"id":"xXDiCyy3Zhp4"},"source":["# Train model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TDj4peFQZKwb","outputId":"edbeb1d8-c578-4987-92f0-c445d76dd518"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0, It: 0, Loss: 6.594e-02, Loss L1: 4.969e-02, Loss NeoHook: 1.625e+03, Time: 8.13\n","Epoch: 0, It: 100, Loss: 2.760e-02, Loss L1: 2.689e-02, Loss NeoHook: 7.077e+01, Time: 91.27\n","Reference image vs predicted image frame 0\n","Image | MSE: 2.83481e-01, SSIM: 0.92028\n","Reference image vs Template image frame 0\n","Image | MSE: 0.00000e+00, SSIM: 1.00000\n","Reference image vs predicted image frame 1\n","Image | MSE: 2.91923e-01, SSIM: 0.91459\n","Reference image vs Template image frame 1\n","Image | MSE: 1.21262e-01, SSIM: 0.98309\n","Reference image vs predicted image frame 2\n","Image | MSE: 3.10167e-01, SSIM: 0.90275\n","Reference image vs Template image frame 2\n","Image | MSE: 2.13433e-01, SSIM: 0.95024\n","Reference image vs predicted image frame 3\n","Image | MSE: 3.34739e-01, SSIM: 0.88813\n","Reference image vs Template image frame 3\n","Image | MSE: 2.82560e-01, SSIM: 0.91597\n","Reference image vs predicted image frame 4\n","Image | MSE: 3.67800e-01, SSIM: 0.86419\n","Reference image vs Template image frame 4\n","Image | MSE: 3.47478e-01, SSIM: 0.87788\n","Reference image vs predicted image frame 5\n","Image | MSE: 3.96859e-01, SSIM: 0.84203\n","Reference image vs Template image frame 5\n","Image | MSE: 3.91044e-01, SSIM: 0.84853\n","Reference image vs predicted image frame 6\n","Image | MSE: 4.14607e-01, SSIM: 0.82869\n","Reference image vs Template image frame 6\n","Image | MSE: 4.15520e-01, SSIM: 0.82949\n","Reference image vs predicted image frame 7\n","Image | MSE: 4.25880e-01, SSIM: 0.82085\n","Reference image vs Template image frame 7\n","Image | MSE: 4.30032e-01, SSIM: 0.81733\n","Reference image vs predicted image frame 8\n","Image | MSE: 4.35661e-01, SSIM: 0.81447\n","Reference image vs Template image frame 8\n","Image | MSE: 4.41730e-01, SSIM: 0.80834\n"]}],"source":["# An exception may occur during backpropagation if J = 0 at some point.\n","\n","try:\n","    # Choose either L1 norm or L2 norm to measure the difference between reference and warped template images\n","\n","    model.train_Adam_L1_NeoHook(nEpoch, mu_NeoHook, size=batch_size)\n","    #model.train_Adam_MSE_NeoHook(nEpoch, mu_NeoHook, size=batch_size) \n","\n","except:\n","    \n","    print('J = 0 at some point. Training has stopped. Predictions are made from last iteration.')\n","\n","finally:\n","\n","    ### Save model ###\n","\n","    str_neu = str(neu)\n","    str_mask = str(reg_mask)\n","    str_batch = str(batch_size)\n","\n","    str_mu_NeoHook = str(mu_NeoHook)\n","    str_lmbmu = str(lmbmu)\n","\n","    str_m = str(m)\n","\n","    model_name = 'model_L1_'+str_mu_NeoHook+'_NH_lmbmu_'+str_lmbmu+'_mask_'+str_mask+'_neu_'+str_neu+'_batch_'+str_batch+'_m_'+str_m+'_sigma'\n","    # model_name = 'model_L2_'+str_mu_NeoHook+'_NH_lmbmu_'+str_lmbmu+'_mask_'+str_mask+'_neu_'+str_neu+'_batch_'+str_batch+'_m_'+str_m+'_sigma'\n","    for s in sigma:\n","        model_name = model_name + '_' + str(s)\n","\n","    model_dir = os.path.join( 'results', 'L1', volunteer, model_name) \n","    #model_dir = os.path.join( 'results', 'L2', volunteer, model_name) \n","\n","    model_save = os.path.join(model_dir, model_name)\n","\n","    if not os.path.exists(model_dir):\n","        os.makedirs(model_dir)\n","\n","    model.saver.save(model.sess, model_save)\n","\n","    t2 = time.time()    \n","\n","    ### Loading data for post-processing ###\n","\n","    # Mesh coords of nodes in the ventricle to be deformed to simulate heart beating\n","    surf_mesh = meshio.read( os.path.join( data_path, volunteer+'_surf_vol.vtk' ))\n","\n","    # Ground truth landmarks. Get landmarks at first frame from observer 1 (in the first frame they are the same for observer 2)\n","    gt_inria_lmks_path = os.path.join(data_path, 'LMKS_GT/INRIA_COORDINATES')\n","    gt_obs1 = os.path.join(gt_inria_lmks_path, 'obs1_groundTruth00.vtk')\n","    lmks_gt_obs1 = read_lmks_mesh(gt_obs1)\n","\n","    # For strains\n","\n","    # Mesh coords\n","    origin = np.mean(surf_mesh.points, axis = 0)\n","    cart_coords = surf_mesh.points - origin\n","    angle = np.arctan2(cart_coords[:,1], cart_coords[:,0])\n","\n","    rho = np.hstack((np.cos(angle)[:,None], np.sin(angle)[:,None]))\n","    theta = np.hstack((-np.sin(angle)[:,None], np.cos(angle)[:,None]))\n","\n","    rho_1 = np.reshape(rho, [-1,1,2])\n","    rho_2 = np.reshape(rho, [-1,2,1])\n","\n","    theta_1 = np.reshape(theta, [-1,1,2])\n","    theta_2 = np.reshape(theta, [-1,2,1])\n","\n","    # Surface at first frame\n","    surf_path = os.path.join(model_dir, 'surfaces')\n","    if not os.path.exists(surf_path):\n","        os.makedirs( surf_path )\n","\n","    num_nodes = len(surf_mesh.points)\n","\n","    file_name = os.path.join(surf_path, 'v0_surf.vtk')\n","    meshio.write(file_name, meshio.Mesh(points=surf_mesh.points, cells={'triangle': surf_mesh.cells_dict['triangle'], 'tetra': surf_mesh.cells_dict['tetra']}, \\\n","                    point_data = {'radStrain':np.zeros([num_nodes]), 'circStrain':np.zeros([num_nodes]), 'longStrain': np.zeros([num_nodes]), 'Jacobian': np.ones([num_nodes])}))\n","\n","    # Landmarks at first frame\n","    lmks_path = os.path.join(model_dir, 'lmks')\n","    if not os.path.exists( lmks_path ):\n","        os.makedirs( lmks_path )\n","\n","    file_name = os.path.join( lmks_path, 'lmks_results{0}.vtk'.format(0))\n","    meshio.write(file_name, meshio.Mesh(points=lmks_gt_obs1, cells={}))\n","\n","    # To generate some basic plots for Reference image vs Predicted image\n","    mse_pred_list = []\n","    ssim_pred_list = []\n","\n","    # To generate some basic plots for Reference image vs Template image\n","    mse_temp_list = []\n","    ssim_temp_list = []\n","\n","    # To generate some basic plots for strains\n","    # Radial\n","    mean_rr = np.zeros([frames])\n","    median_rr = np.zeros([frames])\n","    std_rr = np.zeros([frames])\n","    # Circumferential\n","    mean_cc = np.zeros([frames])\n","    median_cc = np.zeros([frames])\n","    std_cc = np.zeros([frames])\n","    # Longitudinal\n","    mean_ll = np.zeros([frames])\n","    median_ll = np.zeros([frames])\n","    std_ll = np.zeros([frames])\n","\n","    # Warping at different times\n","\n","    for i in range(frames):\n","\n","        # Surface warp\n","        surf_mesh_warp, u1, u2, u3, u1x, u1y, u1z, u2x, u2y, u2z, u3x, u3y, u3z, J = model.surface_deformation(surf_mesh.points, i)\n","\n","        grad_um = np.hstack((u1x, u1y, u1z, u2x, u2y, u2z, u3x, u3y, u3z))\n","        grad_um = np.reshape(grad_um, [-1, 3, 3])\n","        grad_um_t = np.transpose(grad_um, [0, 2, 1])\n","\n","        Em = 1/2 * (grad_um + grad_um_t + np.matmul(grad_um_t, grad_um))\n","\n","        Em_new = Em[:,0:2,0:2]\n","        # Radial strain\n","        Em_rr = np.matmul(rho_1, np.matmul(Em_new, rho_2))\n","        # Circumferential strain\n","        Em_cc = np.matmul(theta_1, np.matmul(Em_new, theta_2))\n","        # Longitudinal strain\n","        Em_ll = Em[:,2,2]\n","\n","        mean_rr[i] = np.mean(Em_rr)\n","        median_rr[i] = np.median(Em_rr)\n","        std_rr[i] = np.std(Em_rr)\n","\n","        mean_cc[i] = np.mean(Em_cc)\n","        median_cc[i] = np.median(Em_cc)\n","        std_cc[i] = np.std(Em_cc)\n","\n","        mean_ll[i] = np.mean(Em_ll)\n","        median_ll[i] = np.median(Em_ll)\n","        std_ll[i] = np.std(Em_ll)\n","\n","        # Save warped surface with strains and jacobian\n","        file_name = os.path.join(surf_path, 'v{0}_surf.vtk'.format(i+1))\n","        meshio.write(file_name, meshio.Mesh(points=surf_mesh_warp, cells={'triangle': surf_mesh.cells_dict['triangle']}, \\\n","                    point_data = {'radStrain':Em_rr, 'circStrain':Em_cc, 'longStrain':Em_ll, 'Jacobian': J}))\n","\n","        # Save warped Landmarks \n","        lmks_warp_obs1_mesh = model.lmks_deformation(lmks_gt_obs1, i)\n","        file_name = os.path.join(lmks_path, 'lmks_results{0}.vtk'.format(i+1))\n","        meshio.write(file_name, meshio.Mesh(points=lmks_warp_obs1_mesh, cells={}))\n","\n","        # Prediction on voxel coordinates\n","\n","        imr_pred, u1_pred, u2_pred, u3_pred, u1x_pred, u1y_pred, u1z_pred, u2x_pred, u2y_pred, u2z_pred, u3x_pred, u3y_pred, u3z_pred, J_pred = model.predict(i)\n","\n","        imt_i = imt[i,:,:,:]\n","        ub_im = np.max([np.max(imr), np.max(imt)])\n","        lb_im = np.min([np.min(imr), np.min(imt)])\n","        imr_resc = (imr - lb_im) / (ub_im - lb_im)\n","        imt_resc = (imt_i - lb_im) / (ub_im - lb_im)\n","\n","        # Comparing predicted image and reference image\n","        print('Reference image vs predicted image frame {0}'.format(i))\n","        compare_images(imr_resc[1:-1], imr_pred[1:-1])\n","        print('Reference image vs Template image frame {0}'.format(i))\n","        compare_images(imr_resc[1:-1], imt_resc[1:-1])\n","\n","        mse_pred_list.append( np.sqrt((np.square(imr_resc[1:-1]-imr_pred[1:-1])).mean() / (np.square(imr_resc[1:-1])).mean()) )\n","        mse_temp_list.append( np.sqrt((np.square(imr_resc[1:-1]-imt_resc[1:-1])).mean() / (np.square(imr_resc[1:-1])).mean()) )\n","        ssim_pred_list.append( ssim(imr_resc[1:-1], imr_pred[1:-1]) )\n","        ssim_temp_list.append( ssim(imr_resc[1:-1], imt_resc[1:-1]) )\n","\n","    np.save( os.path.join(model_dir, 'mse_pred_list.npy'), np.array(mse_pred_list))\n","    np.save( os.path.join(model_dir, 'ssim_pred_list.npy'), np.array(ssim_pred_list))\n","    np.save( os.path.join(model_dir, 'mse_temp_list.npy'), np.array(mse_temp_list))\n","    np.save( os.path.join(model_dir, 'ssim_temp_list.npy'), np.array(ssim_temp_list))\n","\n","    np.save( os.path.join(model_dir, 'loss_registration.npy'), np.array(model.lossit_MSE))\n","    np.save( os.path.join(model_dir, 'loss_nh.npy'), np.array(model.lossit_NeoHook))\n","    np.save( os.path.join(model_dir, 'loss.npy'), np.array(model.lossit_value))\n","\n","    # Generate plots of MSE and SSIM\n","    mse_ssim(model_dir, frames, mse_pred_list, mse_temp_list, ssim_pred_list, ssim_temp_list)\n","\n","    # Generate strain graphs\n","    pinn_strains = np.vstack([mean_rr, median_rr, std_rr, mean_cc, median_cc, std_cc, mean_ll, median_ll, std_ll ])\n","    global_strain(data_path, model_dir, frames, pinn_strains)\n","\n","    box_plots(data_path, model_dir)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hj9PlWbeqy5e"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}